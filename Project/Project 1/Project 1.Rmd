---
title: "Project 1"
author: "Xiang Yang Ng, Muiz Rahemtullah"
date: "April 18, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
##Introduction
Our data is Consumer Price Index for all urban consumers on the purchasing power of the US dollar from January 1950 to January 2018. We obtained the data from the U.S. Bureau of Labor Statistics. THe data is last updated on April 17 2018.

Incorporating all the libraries into our project and setting the directory
```{r}
#downloading all the libraries and setting the directory
setwd("/Users/Lenovo/Desktop/Econ 144/Project 1")
rm(list=ls(all=TRUE))
library(lattice)
library(foreign)
library(MASS)
library(car)
require(stats)
require(stats4)
library(KernSmooth)
library(fastICA)
library(cluster)
library(leaps)
library(mgcv)
library(rpart)
library(pan)
library(mgcv)
library(DAAG)
library(tis)
require(datasets)
require(graphics)
library("forecast")

```

Feeding in the data
```{r}
#Obtain the data, clean it and construct the time series for the CPI
data = read.csv("Consumer Price Index for All Urban Consumers Purchasing Power of the Consumer Dollar 1950_1-2018_1.csv")
colnames(data)[2] = "CPI"
attach(data)
CPI_ts = ts(CPI, start = 1950, freq = 12)

```

a) and b)
```{r}
#Plotting both the time series of the CPI and it's difference of the log
plot(CPI_ts, main = "Plot of time series of CPI values", ylab = "CPI")
CPI_ts_diff = diff(log(CPI_ts))
plot(CPI_ts_diff, main = "Plot of time series of CPI percentage change", ylab = "CPI percentage change")
```

The original plot shows that the time series is not covariance stationary since the variance at each specific time point varies. However if we take a difference of the log of the time series, we could clearly see that this difference is covariance stationary.

c)
```{r}
#Plotting the ACF and the PACF of the time series
acf(CPI_ts)
pacf(CPI)
```

The ACF shows that there is significant correlation for lag 1, which decreases as the lag increases. This means that subsequence lags have correlation which only depends on the first lag. This is shown by the PACF, which is 1 for the first lag and zero otherwise.

d)
```{r}
#we regress time series of the CPI with a linear model and nonlinear models such as quadratic, logatithmatic, exponential, log-linear 

t = seq(1950, 2018, length = length(CPI_ts))
y1 = lm(CPI_ts~t)

y2 = lm(CPI_ts~t+I(t^2))

y3=lm(log(CPI_ts) ~ t)

ds=data.frame(x=t, y=CPI_ts)
y4=nls(y ~ exp(a + b * t),data=ds, start = list(a = 0, b = 0))

y5=lm(CPI_ts ~ log(t))

#plotting the graphs of the time series values with the regression fitted values for all of the models 
matplot(t, cbind(y1$fitted.values, CPI), type = 'l', xlab = "Time", ylab = "Values", lwd = 0.1)
title(main = "Linear model")
legend("topright", legend = c("Time Series Values", "Regression fitted values"), col = c("red", "black"), lty = c(2,1))

matplot(t, cbind(y2$fitted.values, CPI), type = 'l', xlab = "Time", ylab = "Values", lwd = 0.1)
title(main = "Quadratic model")
legend("topright", legend = c("Time Series Values", "Regression fitted values"), col = c("red", "black"), lty = c(2,1))

matplot(t, cbind(y3$fitted.values, CPI), type = 'l',xlab = "Time", ylab = "Values")
title(main = "Log-linear model")
legend("topright", legend = c("Time Series Values", "Regression fitted values"), col = c("red", "black"), lty = c(2,1))

matplot(t, cbind(predict(y4, list(x = ds$x)), CPI), type = 'l',xlab = "Time", ylab = "Values")
title(main = "Exponential model")
legend("topright", legend = c("Time Series Values", "Regression fitted values"), col = c("red", "black"), lty = c(2,1))

matplot(t, cbind(y5$fitted.values, CPI), type = 'l',xlab = "Time", ylab = "Values")
title(main = "Linear-log model")
legend("topright", legend = c("Time Series Values", "Regression fitted values"), col = c("red", "black"), lty = c(2,1))
```

All of the lines, except the log-linear, seems to follow the trend of the data well. This means that these models fit the data well. We have to see the summary for the log-linear to determine whether the model fits well to the data.

e)
```{r}
#Plotting the scatterplot of the residuals of all the models
plot(resid(y1), fitted(y1), xlab = "Residuals",
    ylab = "Fitted Values", main = "Linear model",
    pch = 19, col = "navyblue")
plot(resid(y2), fitted(y2), xlab = "Residuals",
    ylab = "Fitted Values", main = "Quadratic model",
    pch = 19, col = "navyblue")
plot(resid(y3), fitted(y3), xlab = "Residuals",
    ylab = "Fitted Values", main = "Log-linear model",
    pch = 19, col = "navyblue")
plot(resid(y4), fitted(y3), xlab = "Residuals",
    ylab = "Fitted Values", main = "Exponential model",
    pch = 19, col = "navyblue")
plot(resid(y5), fitted(y3), xlab = "Residuals",
    ylab = "Fitted Values", main = "Linear-log model",
    pch = 19, col = "navyblue")
```

For all of the models, the residuals follow a non-linear trend, which means that there are still dynamics in the data that we have not accounted for. For both the linear and the linear-log models, there seems to be a little more residuals on the right, which indicate that the models underpredict the data, while for both the quadratic and exponential models, there seems to be little more residuals on the left, indicating that the models overpredict the data. The residuals are most balanced on both sides are from the log-linear model.

f)
```{r}
#Plotting the histogram of the residuals of all the models
truehist(resid(y1), main = "Linear", col = "slategrey",
    xlab = "Residuals", ylab = "Standardized Unit")
truehist(resid(y2), main = "Quadratic", col = "slategrey",
    xlab = "Residuals", ylab = "Standardized Unit")
truehist(resid(y3), main = "Log-linear", col = "slategrey",
    xlab = "Residuals", ylab = "Standardized Unit")
truehist(resid(y4), main = "Exponential", col = "slategrey",
    xlab = "Residuals", ylab = "Standardized Unit")
truehist(resid(y5), main = "Linear-log", col = "slategrey",
    xlab = "Residuals", ylab = "Standardized Unit")
```

These histograms fully mimics the plot from e). The analysis for each model should then be the same.

f)
```{r}
#Displaying the summary of the regression for each model
summary(y1)
summary(y2)
summary(y3)
summary(y4)
summary(y5)
```

All the models fit the data pretty well, with at least a 0.85 for the R^2. From the t- and F-distribution, we can see that the time for all models are also statistically significant, indicating that time plays a significant role in the data. Among all the models, the quadratic and the log-linear models have the best R^2, which might indicate that they are the best models to fit the data.

h)
```{r}
#Calculating the AIC and the BIC of the models
AIC(y1,y2,y3,y4,y5)
BIC(y1,y2,y3,y4,y5)
```

We can see that the log linear model is the best model to work with as the AIC and the BIC values are the smallest for the log linear model.

i)
```{r}
#Plotting the fitted values, Confidence and Prediction intervals for 60 periods ahead, i.e predicting for the years 2018-2023.
time <- data.frame(t = seq(2018,2023))
pred <- predict(lm(CPI~t), time, se.fit = TRUE)
pred_interval <- predict(lm(log(CPI)~t), time, level = .95, interval = "prediction")
conf_interval <- predict(lm(log(CPI)~t), time, level = .95, interval = "confidence")

matplot(time$t, cbind(conf_interval, pred_interval[,-1]), lty = c(1,1,1,3,3),col = c("black", "blue", "blue", "red", "red"), type = "l", lwd = 2, ylab = "Predicted CPI", xlab = "Time", main = "Confidence and Prediction Intervals")
legend("topleft", legend = c("Confidence Interval", "Prediction Interval", "Fit"), col = c("red", "blue", "black"), lty = c(2,1,1), cex = .8)
```


We forecast the data using the our log-linear model for 60 periods ahead or for 5 years ahead from 2018-2023. We can see that the over time, the purchasing power of the US dollar will continue to fall slightly. This makes sense as it had been falling slightly until the present time. 

##Problem 2
a)
```{r}
#Run a time series regression on the season and show the results
fit_season=tslm(CPI_ts ~ season)
summary(fit_season)
```

All the season dummies' coefficients are statistically insignificant, as evidenced by the high p-values for both the t- and F-statistics. This shows that seasonality weighs little on the data.

b)
```{r}
#Plotting the season factors
plot(fit_season$coef,type='l',ylab='Seasonal Factors', xlab="Season",lwd=2, main="Plot of Seasonal Factors")

```

The plot of seasonal factors show that there is little significance of seasonal factors on the data. That further strengthens the idea that the trend dominates the seasonal elements.

(c)
```{r}
#First construct the time series regression model for both the trend and season
#Then plot the residuals vs fitted values graphs
fit_full = tslm(log(CPI_ts) ~ trend + season)
plot(fit_full$residuals,fit_full$fitted.values, main = "Residuals vs Fitted values", xlab = "Residuals", ylab = "Fitted values")
```

There seems to be a nonlinear pattern in the plot, which means that the model is heterskedastic and thus we have not accounted for all the dynamics in the data. But at the very least the number of residuals on the right seems to balance off with the number on the left, which means that model predicts the data quite well.

(d)
```{r}
#Producing the summary of the regression
summary(fit_full)
```

Only the trend's coefficient is statistically significant, while the seasons' coefficients are all statistically insignificant based on the t-values and it's corresponding probabilities. This suggests that there is little seasonality in the data as trend weighs more on the data than seasonality. Since the R^2 is very high, we can see that the model fits the data well. The large F-statistics, paired with a significantly low p-value, shows that all the regression coefficients are not equal zero. But this is because of the trend coefficient having large t-value and a corresponding low p-value that shows it is a statistically significant variable not equal zero.

e)
```{r}
#Plotting the forecast of the data using the full model for 60 periods ahead
plot(forecast(fit_full,h=60),main="Forecast Trend + Seasonality")
lines(fit_full$fitted.values, col="red")

#Plotting the ets of the CPI_ts and finding the accuracy
fit_ets = ets(CPI_ts)
plot(fit_ets)
accuracy(fit_ets)
plot(forecast(fit_ets,level=c(50,80,95)))
```

We forecast the data using the our full model for 60 periods ahead or for 5 years ahead from 2018-2023. As we can see, just plotting the forecast of the full model yields very bad result. However, by using the ETS function, we manage to improve the forecast and produce the graphs that shows a better picture of the forecast. Both forecasts show that the CPI is trending downwards.

##Conclusion
  After testing with several models, we think that the log-linear model seems to fit the data best. However, looking at the residuals vs fitted values plot, the model is still heteroskedastic. 

  Forecasting both the log-linear model and the log-linear+seasonality yields very similar results, which means that seasonality seems to play very little role on the data.
  
  Moving forward, we would have to add more dynamics to the log-linear model, such as incorporating ARMA to our model, to better improve the fit of the data.
  
##References/Citation
U.S. Bureau of Labor Statistics, Consumer Price Index for All Urban Consumers: Purchasing Power of the Consumer Dollar [CUUR0000SA0R], retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/CUUR0000SA0R, April 25, 2018.