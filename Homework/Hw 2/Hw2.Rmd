---
title: "hw2"
author: "Xiang Yang Ng"
date: "April 15, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Set your 'working directory' to the folder where all the data and respective codes are located.

```{r}
setwd("/Users/Lenovo/Desktop/Econ 144/Homework/Hw 2")
```

Clear variables and prior sessions
```{r}
rm(list=ls(all=TRUE))
```

Download all the libraries
```{r}
library(lattice)
library(foreign)
library(MASS)
library(car)
require(stats)
require(stats4)
library(KernSmooth)
library(fastICA)
library(cluster)
library(leaps)
library(mgcv)
library(rpart)
library(pan)
library(mgcv)
library(DAAG)
library(tis)
require(datasets)
require(graphics)
library("forecast")

#require(astsa)
library(xtable)
library(stats)
```
##Problem 1

a)
```{r}
#reading in the data and change the names
z=read.table("labordata.dat")
names(z)= c("male","female","total")
attach(z)

# Convert data to time series format:
male_ts<-ts(male,start=1948,freq=12)
female_ts <- ts(female,start=1948,freq=12)
total_ts <- ts(total,start=1948,freq=12)
t<-seq(1948, 1991,length=length(male_ts))

matplot(t, cbind(male_ts, female_ts, total_ts), xlab = "Time", ylab = "Values",  type="l", lwd=2, col = c("black", "blue", "red"))
title(main = "Time series values against time")
legend("topleft", legend=c("Female", "Total", "Male"),
       col=c("blue", "red", "black"), cex=0.8, lty = 1)

```

b)
```{r}
#Linear Model
y1 = lm(female_ts~t)
matplot(t, cbind(y1$fitted.values, female_ts), xlab = "Time", ylab = "Values",  type="l", lwd=2, col = c("black", "blue"))
title(main = "Linear model")
legend("topleft", legend=c("Regression Fitted Values", "Time Series"),
       col=c("black", "blue"), cex=0.8, lty = 1)

#Quadratic Model
y2 = lm(female_ts~t+I(t^2))
matplot(t, cbind(y2$fitted.values, female_ts), xlab = "Time", ylab = "Values",  type="l", lwd=2, col = c("black", "blue"))
title(main = "Quadratic model")
legend("topleft", legend=c("Regression Fitted Values", "Time Series"),
       col=c("black", "blue"), cex=0.8, lty = 1)

#Exponential Model
ds=data.frame(x=t, y=female_ts)

#using a nonlinear square
y3=nls(y ~ exp(a + b * t),data=ds, start = list(a = 0, b = 0))

matplot(t, cbind(predict(y3, list(x = ds$x)), female_ts), xlab = "Time", ylab = "Values",  type="l", lwd=2, col = c("black", "blue"))
title(main = "Exponential model")
legend("topleft", legend=c("Regression Fitted Values", "Time Series"),
       col=c("black", "blue"), cex=0.8, lty = 1)


```

All 3 models seem to fit the data pretty well. However the quadratic and exponential models fit the data better than the linear one.

c)
```{r}
plot(y1$fitted.values, y1$residuals, main = "Linear:Residual vs Fitted Values", ylab = "Residuals", xlab = "Fitted values")

plot(y2$fitted.values, y2$residuals, main = "Quadratic:Residual vs Fitted Values", ylab = "Residuals", xlab = "Fitted values")

plot(predict(y3, list(x = ds$x)), residuals(y3), main = "Exponential:Residual vs Fitted Values", ylab = "Residuals", xlab = "Fitted values")

```

There seems to be a nonlinear pattern in the plot of residuals vs fitted values for the linear model, which suggest that indeed the data must be represented by a nonlinear model. There seems to be more negative residuals in the quadratic plot, which implies that the model overpredicts the fitted values, while the residuals are more or less balanced in positive and negative direction for the exponential model.

d)
```{r}
#Calculating the AIC and BIC 
AIC(y1,y2,y3)
BIC(y1,y2,y3)
```

The AIC and the BIC seems to show that the quadratic model is the better fit for the data.

d)
```{r}
#Plotting the graph with the 95% confidence and prediction intervals
tn=data.frame(t=seq(1992,2002))
pred=predict(lm(female_ts ~ t), tn, se.fit = TRUE)
#plot(c(male_ts,pred$fit),type='l',xlim=c(1940,2000))
pred.plim = predict(lm(female_ts ~ t),tn, level =0.95, interval="prediction")
pred.clim = predict(lm(female_ts ~ t), tn,level=0.95, interval="confidence")
matplot(tn$t,cbind(pred.clim, pred.plim[,-1]),
        lty=c(1,2,2,3,3), type="l", lwd=2, ylab="predicted y",xlab="Time", col =         c("red", "black", "black", "blue","blue"))
title(main = "Predicted value vs Time")
legend("topright", legend = c("Prediction interval", "Confidence Interval", "Forecasted fit"), lty = c(3,2,1), col = c("blue", "black", "red"))
```

The plot shows that there is a upward trend of the female participation rate. The small interval provides strong evidence of the upward trend, since it indicates that the values are not going to vary by much.

f)
```{r}
fit_hw = HoltWinters(female_ts)
hwpred = predict(fit_hw, ahead = 120, prediction.interval = T, level = 0.5)
plot(fit_hw, hwpred, ylab="Participation Rate (Male)", xlab="Time", lwd=1, col='black',xlim=c(1948,1999))
```

The holt-winters model is much better than the my model fitting the data.

g)
```{r}
#Create a prediction time series and then plot it 
pred_hw=predict(fit_hw, 144, prediction.interval=T, level =0.95)
t2 = seq(from = 1991, to = 2002, length = length(pred_hw[,1]))
matplot(t2, cbind(pred_hw), col = c("black", "blue", "blue"), type = "l", lty = c(1,2,2), xlab = "Time", ylab = "Forecasted value+Prediction Interval")
title(main = "Plot of forecasted Holt-Winter")
legend("topleft", legend=c("Forecasted fit", "Interval"), lty = c(1,2), col = c("black", "blue"))
```

Just like the previous forecast, the Holt-Winters also predict an upward trend.


##Problem 2
Clearing previous problem's data and obtaining the data for this problem
```{r}
rm(list=ls(all=T))
data = read.csv("3.2.csv")
attach(data)
```

Calculate the monthly inflation and the ex post interest rate
```{r}
month_inf = diff(log(cpi))
ex_post_IR = nomrate - month_inf
```


Obtain the data and get the growth rates of the personal real expenditure and income
```{r}
data1 = read.csv("3.1.csv", header = T)
attach(data1)
rpce_pctchg = diff(log(rpce))
rdpi_pctchg = diff(log(rdpi))

y = lm(rpce_pctchg~rdpi_pctchg+ex_post_IR[-length(ex_post_IR)])
summary(y)
```

The ex post interest rate is statistically insignicant since it has a high p-value corresponding to the t-value. This means that interest rates do not affect consumption significantly.

##Problem 3
Clearing previous problem's data and obtaining the data for this problem
```{r}
rm(list=ls(all=T))
data = read.csv("4.4.csv")
names(data) = c("time", "house_price", "interest_rate")
attach(data)
```

Creating the lags
```{r}
hp_lag1 = append(NA, house_price[-length(house_price)])
hp_lag2 = append(c(NA, NA), house_price[-c(length(house_price)-1, length(house_price))])
hp_lag3 = append(c(NA,NA,NA), house_price[-c(length(house_price)-2, length(house_price)-1, length(house_price))])
hp_lag4 = append(c(NA,NA,NA,NA), house_price[-c(length(house_price)-3, length(house_price)-2, length(house_price)-1, length(house_price))])

house_price_ts = ts(house_price, start=1980+(1/4), freq=4)
hp_lag1k = append(NA, house_price_ts[-length(house_price_ts)])
hp_lag2k = append(c(NA, NA), house_price_ts[-c(length(house_price_ts)-1, length(house_price_ts))])
```

Running the regression
```{r}
y1 = lm(house_price~hp_lag1)
summary(y1)

y2 = lm(house_price~hp_lag1+hp_lag2)
summary(y2)

y3 = lm(house_price~hp_lag1+hp_lag2+hp_lag3)
summary(y3)

y4 = lm(house_price~hp_lag1+hp_lag2+hp_lag3+hp_lag4)
summary(y4)

y5 = lm(house_price_ts~hp_lag1k+hp_lag2k)
summary(y5)
```


##Problem 4
Clearing previous problem's data and obtaining the data for this problem
```{r}
rm(list=ls(all=T))
data = read.csv("4.41.csv")
names(data) = c("data", "actual_rgdp_growth", "actual_greenbook_growth")
attach(data)
```

Forecast error
```{r}
forecast_err = actual_rgdp_growth-actual_greenbook_growth
```

Plot the time series of realized values, forecasts, and forecast errors.
```{r}
realized_ts = ts(actual_rgdp_growth, start = 1969, freq = 4)
forecast_ts = ts(actual_greenbook_growth, start = 1969, freq = 4)
forecast_error_ts = ts(forecast_err, start = 1969, freq = 4)

plot(realized_ts, main = "Plot of realized values")
plot(forecast_ts, main = "Plot of forecasted values")
plot(forecast_error_ts, main = "Plot of forecasted errors")
```

All 3 graphs might be first-order weakly stationary, with the second one least likely to be, since there is a sharp decline near the year 1980. It is very unlikely that the graphs are covariance stationary since the values vary more during the first half of the period.

Compute the descriptive statistics and the boxplots
```{r}
summary(realized_ts)
summary(forecast_ts)
summary(forecast_error_ts)

boxplot(realized_ts, forecast_ts, forecast_error_ts, main = "The boxplots of the realized values, \n forecasted values and forecasted error")
```

The boxplots show that there is slightly a larger spread for the realized values and the forecasted errors but also more outliers than the forecasted values.

Compute the ACF and PACF of the realized values, forecasts, and
forecast errors and also the Ljung-box test for the forecasted values:
The first ACF and PACF are of the realized values, followed by forecasted values and forecasted errors
```{r}
acf(realized_ts)
pacf(realized_ts)

acf(forecast_ts)
pacf(forecast_ts)

acf(forecast_error_ts)
pacf(forecast_error_ts)

for (i in 1:20){
  print(Box.test(realized_ts, lag = i, type = "Ljung-Box"))
}

for (i in 1:20){
  print(Box.test(forecast_ts, lag = i, type = "Ljung-Box"))
}

for (i in 1:20){
  print(Box.test(forecast_error_ts, lag = i, type = "Ljung-Box"))
}

```

According to the ACF and PACF of the realized values, there is a significant correlation in the 2nd and the 3rd lags, while those of the forecasted errors, there is significant correlation in the 3rd lag. This means that there is definitely a significant correlation for the 3rd lag. From the Ljung-Box test, there seems to be significant time dependence for the realized values and the forecasted values, but not the forecasted errors, which means that the forecasted errors are random.

##Problem 5
Clearing previous problem's data and obtaining the data for this problem
```{r}
rm(list=ls(all=T))
data = read.csv("5.4.csv")
data$X = NULL
names(data)=c("data_AMEX", "AMEX", "data_SP500", "SP500")
attach(data)

```

Taking the first difference for both the AMEX and the SP500 data to create the returns and then create a time series objects out of them
```{r}
AMEX_diff = diff(AMEX)
SP500_diff = diff(SP500)

#y1 = time series for AMEX and y2 = time series for SP500
y1 = ts(AMEX_diff, start=1995+(249/252), freq = 252)
y2 = ts(SP500_diff, start = 1990+(13/252), freq = 252)

#need to get rid of the NA's in the y1 and y2 data
y1 = y1[complete.cases(y1)]
y2 = y2[complete.cases(y2)]
```

Calculating the ACF and PACF for both of the returns. First ACF and PACF belongs to the returns of AMEX, followed by the SP500.
```{r}
acf(y1)
pacf(y1)
acf(y2)
pacf(y2)
```

According to the ACF and PACF, there seems to be seasonality in the data for both AMEX and SP500. But more importantly we also see a lot more signicant autocorrelations for several different lag values. This might be because of the fact that there are a lot more data points that we are using, and thus we can see the autocorrelations more defined. 
