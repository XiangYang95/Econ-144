---
title: "Hw 5"
author: "Xiang Yang Ng"
date: "June 2, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Downloading all the libraries and set the working directory
```{r}
setwd("/Users/Xiang/OneDrive/Desktop/Econ-144/Homework/Hw 5")
library(lattice)
library(foreign)
library(MASS)
library(car)
require(stats)
require(stats4)
library(KernSmooth)
library(fastICA)
library(cluster)
library(leaps)
library(mgcv)
library(rpart)
library(pan)
library(mgcv)
library(DAAG)
library("TTR")
library(tis)
require("datasets")
require(graphics)
library("forecast")
#install.packages("astsa")
#require(astsa)
library(fpp)
library(strucchange)

library(tseries)
library(rugarch)
```

##Problem 1
```{r}
#Clear the previous environment 
rm(list = ls(all=T))
library(fGarch)
# model=ugarchspec(
# variance.model = list(model = "sGARCH", garchOrder = c(2, 2)),
# mean.model = list(armaOrder = c(2, 2), include.mean = TRUE),
# distribution.model = "sstd")
# 
# #Sanity check: explore the model parameters
# #mxreg is the number of external regressors
# model@model$pars

#Crete the low and high persistence GARCH simulations
lowPer = garchSim(spec = garchSpec(model = list(alpha = 0.2, beta = 0.3)), n = 1000, n.start = 100, extended = TRUE)
highPer = garchSim(spec = garchSpec(model = list(alpha = 0.1, beta = 0.88)), n = 1000, n.start = 100, extended = TRUE)

#plotting the scatterplots and histograms of the low and persistence simulations
plot(lowPer$garch, main = "Plot of low persistence time series", xlab = "Values")
plot(lowPer$sigma^2, main = "Plot of low persistence conditional variance", xlab = "Values")
plot(highPer$garch, main = "Plot of high persistence time series", xlab = "Values")
plot(highPer$sigma^2, main = "Plot of high persistence conditional variance", xlab = "Values")

hist(lowPer$garch, main = "Histogram of low persistence time series")
hist(lowPer$garch/lowPer$sigma, main = "Histogram of low persistence standardized time series")

hist(highPer$garch, main = "Histogram of high persistence time series")
hist(highPer$eps/highPer$sigma, main = "Histogram of high persistence standardized time series")
```
Looking at the plots of low persistence, we see that the time series plot is scattered rather evenly and the conditional variance plot is less volatile, compared to those of the high persistent plots. The histograms of both low and high persistence time series plots show that it cannot be modeled as a normal distribution, but after standardizing, it seems that it can be modeled by a normal distribution.

```{r}
tsdisplay(lowPer$garch, main = "ACF and PACF of the low persistence time series")
tsdisplay(lowPer$garch^2, main = "ACF and PACF of the low persistence squared time series")
tsdisplay(lowPer$garch/lowPer$sigma, main = "ACF and PACF of the low persistence standardized time series")
tsdisplay((lowPer$garch/lowPer$sigma)^2, main = "ACF and PACF of the low persistence standardized squared time series")
tsdisplay(highPer$garch, main = "ACF and PACF of the high persistence time series")
tsdisplay(highPer$garch^2, main = "ACF and PACF of the high persistence squared time series")
tsdisplay(highPer$garch/highPer$sigma, main = "ACF and PACF of the high persistence standardized time series")
tsdisplay((highPer$garch/highPer$sigma)^2, main = "ACF and PACF of the high persistence standardized squared time series")
```
For both the low and high persistence ACF and PACF of the time series, we see that there is almost no spikes. But after squaring them, we see that there are spikes that are apparent, especially the high persistence one. Standardizing them thus eliminates those spikes.

##Problem 2
```{r}
#Clear the pervious environment and download the SP500 data
library(openxlsx)
rm(list = ls(all=T))
data = read.xlsx("Chapter14_exercises_data.xlsx", sheet = "Exercise 3, 4, 10")
```

Obtain the time series and plot the volatility
```{r}
close_ts = ts(data$Close, start = 2000, freq = 252)
plot(volatility(close_ts, n = 10, calc = "close", N = 252, mean0 = FALSE), ylab = "Values", main = "Plot of volatility of SP500")
```

The volatility from the past is less volatile compared to more recent times. This means that the errors are heteroskedastic. 

Compute the ACF and PACF of the returns and the squred returns
```{r}
close_returns_ts = diff(log(close_ts))
tsdisplay(close_returns_ts, main = "Plot of the returns and its ACF and PACF")
tsdisplay(close_returns_ts^2, main = "Plot of the squared returns and its ACF and PACF")
```

The ACF and PACF of the returns seem to have not much spikes, but after squaring, the spikes become more apparent, which is evident of heteroskedastic errors.

From the PACF of the squared returns, we first fit an ARCH(14) to see if this wipes out the dynamics
```{r}
model=ugarchspec(
variance.model = list(model = "sGARCH", garchOrder = c(14, 0)),
mean.model = list(armaOrder = c(1, 2), include.mean = TRUE),
distribution.model = "sstd")

#Sanity check: explore the model parameters
model@model$pars
```
```{R}
# Fit the model to the data
modelfit=ugarchfit(spec=model,data=close_returns_ts)
modelfit
```

```{r}
plot(modelfit, which = 10)
plot(modelfit, which = 11)
```

From the plot of the standardized residuals' ACF and PACF, we can see that all of the dynamics have been wiped out, which means that the model is good for fitting the data.

Alternatively, we can try a GARCH(2,1) model
```{r}
model1=ugarchspec(
variance.model = list(model = "sGARCH", garchOrder = c(2, 1)),
mean.model = list(armaOrder = c(1, 2), include.mean = TRUE),
distribution.model = "sstd")

#Sanity check: explore the model parameters
model1@model$pars
```

```{r}
# Fit the model to the data
modelfit1=ugarchfit(spec=model1,data=close_returns_ts)
modelfit1
```
```{r}
plot(modelfit1, which = 10)
plot(modelfit1, which = 11)
```

We see that the model fits the data equally well looking at how the standardized residuals' ACF and PACF have almost no spikes. This means that a GARCH model is better than just an ARCH model considering GARCH models require less parameters than ARCH models.

##Problem 3
1- and 2-step ahead forecasts of the volatility
```{r}
modelfor = ugarchforecast(modelfit1, data = NULL, n.ahead = 2, n.roll = 0, out.sample = 0)
print("Volatility forecast")
modelfor@forecast$sigmaFor
low95 = modelfor@forecast$seriesFor - 1.96*modelfor@forecast$sigmaFor
high95 = modelfor@forecast$seriesFor + 1.96*modelfor@forecast$sigmaFor
forecasted_values = data.frame(low95, modelfor@forecast$seriesFor ,high95)
colnames(forecasted_values) = c("Low95", "Forecasted value", "High95")
print("Returns forecast")
forecasted_values
```

##Problem 4
```{r}
#clear previous environment and obtain NYSE closing price
library(quantmod)
rm(list = ls(all=T))
getSymbols("NYA")
```

a)
Get the AR model of the data
```{r}
NYSE_returns_ts = diff(log(NYA$NYA.Close[complete.cases(NYA$NYA.Close)]))
NYSE_returns_ts = NYSE_returns_ts[complete.cases(NYSE_returns_ts)]
armodel = ar(NYSE_returns_ts)
summary(armodel)
armodel
```

From here we get that the order of an AR model fitting the data is 34

Now we will fit a GARCH(1,1) model
```{r}
model=ugarchspec(
variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
mean.model = list(armaOrder = c(0, 0), include.mean = TRUE),
distribution.model = "sstd")

# Fit the model to the data
modelfit=ugarchfit(spec=model,data=residuals(armodel)[complete.cases(residuals(armodel))])
modelfit
```

Check the ACF and PACF of the returns and the squared standardized residuals
```{r}
plot(modelfit, which = 10)
plot(modelfit, which = 11)
```

We can see that almost all the dynamics of the data has been wiped out and we also took care of the heteroskedasticity of the errors.

b)fitting both the AR model and GARCH together(but since the AR model takes too many orders, we use an ARMA model instead). After using auto.arima, we find that the optimal ARMA model is 1,1 respectively.
```{r}
model=ugarchspec(
variance.model = list(model = "sGARCH", garchOrder = c(1, 1)),
mean.model = list(armaOrder = c(1, 1), include.mean = TRUE),
distribution.model = "sstd")

# Fit the model to the data
modelfit=ugarchfit(spec=model,data= NYSE_returns_ts)
modelfit
```

Plot the ACF of the residuals and squared residuals
```{r}
plot(modelfit, which = 10)
plot(modelfit, which = 11)
```

We can see again that the dynamics are almost wiped off since there are not many spikes in the residuals.

##Problem 5
Note: we don't want to clear previous environment since we still want to work on the same data sets.
a)
```{r}
#Fit an AR(5) model
ar5 = ar(NYSE_returns_ts, order.max = 5, aic = FALSE)
# Standard deviation of the spread

plot(sqrt(252) * runSD(fitted(ar5)[complete.cases(fitted(ar5))], 10), main = "Volatility from the fitted AR(5) model's fitted values", ylab = "Values")
```

As we can see, the standard deviation spikes in the past periods and slowly stabilizes as time pases. This means that we still have not taken into account the heteroskedasticity of the errors.

b)
Fitting an exponential smoothing model to the data
```{r}
esModel = ets(NYSE_returns_ts)

plot(sqrt(252) * runSD(fitted(esModel)[complete.cases(fitted(esModel))], 10), main = "Volatility from the exponentional smoothing model's fitted values", ylab = "Values")
```

The volatility plot looks similar as in (a), i.e the errors are heteroskedastic.

c) Fitting a GARCH model that we used in problem 4
```{r}
plot(sqrt(252) * runSD(fitted(modelfit)[complete.cases(fitted(modelfit))], 10), main = "Volatility from the GARCH model's fitted values", ylab = "Values")
```

Interestingly enough, the volatitility plot of the ARMA+GARCH model would yield the same shape.

d)
Even though the shape of the volatility series is similar for all 3, the values are not the same. The GARCH model is more preferable, since the GARCH model takes into account that the variance might be varying, which would create a more accurate picture of the volatility plot, unlike the AR(5) and exponential smoothing models.

##Problem 6
a) The model is ARCH(1): y_t = 76 + eps_t
                         sigsqr^2 = 3 + 0.6 eps_(t-1)^2
```{r}
rm(list = ls(all=T))
#if y_t = 92
eps_t = 92-76
sigsqr1 = 3 + 0.6*(eps_t)^2
sigsqr = vector()
sigsqr[1] = sigsqr1
sigsqr[2] = 3 + 0.6*sigsqr[1]
alpha = 0.6
for (i in 3:100){
  sigsqr[i] = 0.6^(i-1)*sigsqr[i-1]
  for (j in 0:i-2){
    sigsqr[i] = sigsqr[i] + alpha^j
  }
}
print("The forecast of the conditional variance for the next 10 days")
sigsqr[1:10]
```

b) This question requires us to find the 90% confidence interval for the value. So [72,80] = 76 +/- 1.645*sigma
```{r}
sig = (80-76)/1.645
print("The required conditional variance")
sig
print ("The forecast of the conditional variance for the next 100 days")
sigsqr
```

We can see that the conditional variance converges to 6.944 as the period increases to infinity. This means that the conditional variance will never drop to the level so that there is a 90% probablity that the temperature will stay between 72 and 80.

c)
Given our current constraint, we can't forecast a bad weather since the conditional forecast converges to a constant even after 20 days.
